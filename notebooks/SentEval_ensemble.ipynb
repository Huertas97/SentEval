{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SentEval_ensemble.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Huertas97/SentEval/blob/master/notebooks/SentEval_ensemble.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xgcvauSg5brc"
      },
      "source": [
        "<font color=\"orange\" size=6>Information</font>\n",
        "\n",
        "We are going to evaluate the ensemble of the multilinguals Sentence Transformer  models without dimensional reduction over the different task of SentEval dataset (https://github.com/facebookresearch/SentEval). \n",
        "\n",
        "\n",
        "<font color=\"orange\">We are gonna evaluate the following models on SentEval: </font>.\n",
        "\n",
        "1. Each model separately without PCA\n",
        "  * distiluse-base-multilingual-cased\n",
        "  * xlm-r-distilroberta-base-paraphrase-v1\n",
        "  * xlm-r-bert-base-nli-stsb-mean-tokens\n",
        "  * LaBSE\n",
        "  * distilbert-multilingual-nli-stsb-quora-ranking\n",
        "\n",
        "\n",
        "2. The analogous ensemble combination of models with PCA over STS benchmark 2017 Multilingual without PCA \n",
        "  * Ensemble of 5 models (without the PCA)  \n",
        "\n",
        "\n",
        "3. The analogous ensemble combination of models with PCA with best ratio result / nº dim over STS benchmark 2017 Multilingual without PCA \n",
        "\n",
        "  * Combination of 2 models without PCA:  xlm-r-bert-base-nli-stsb-mean-tokens, xlm-r-distilroberta-base-paraphrase-v1\n",
        "\n",
        "<br>\n",
        "<font  size=5>Important</font>\n",
        "\n",
        "\n",
        "\n",
        "The results from STS Benchmark 2017 can show slight differences. That because how is computed the cosine similarity. We have used `1 - paired_cosine_distances ` from sklearn.metrics.pairwise [(+ info)](https://stackoverflow.com/questions/36998330/cosine-similarity-output-different-for-different-libraries).\n",
        "\n",
        "Meanwhile SentEval uses \n",
        "\n",
        "\n",
        "```\n",
        "def cosine(u, v):\n",
        "    return np.dot(u, v) / (np.linalg.norm(u) * np.linalg.norm(v))\n",
        "```\n",
        "\n",
        "\n",
        "The difference resides in the rounded result. \n",
        "Our approach (pairwise distance) gives just one decimal. Meanwhile, SentEval gives more.  \n",
        "<br>\n",
        "<font size=5>Important</font>\n",
        "\n",
        "ImageCaptionRetrieval and SNLI are highly computational consumming. Thus, we do not consider this task. \n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jLkIBwJYIs5A"
      },
      "source": [
        "!pip install -U -q sentence_transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMIUd7ug9-HM"
      },
      "source": [
        "# Loading SentEval"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVF9MjUi5X2z"
      },
      "source": [
        "# Clone the repository and all the dependencies\n",
        "!git clone https://github.com/Huertas97/SentEval.git\n",
        "\n",
        "# Download the data\n",
        "%cd /content/SentEval/data/downstream\n",
        "!bash ./get_transfer_data.bash"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "319IbumH_H4j"
      },
      "source": [
        "# Each model separately without PCA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cVk5u3JtrOxP"
      },
      "source": [
        "## distiluse-base-multilingual-cased"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0cPehFTJe_Jn"
      },
      "source": [
        "%cd\n",
        "%cd /content/SentEval/examples\n",
        "!python ensemble.py --models distiluse-base-multilingual-cased"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vd9ouTHiz8Qr"
      },
      "source": [
        "## xlm-r-distilroberta-base-paraphrase-v1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8yLcDPnz8rI"
      },
      "source": [
        "%cd\n",
        "%cd /content/SentEval/examples\n",
        "!python ensemble.py --models xlm-r-distilroberta-base-paraphrase-v1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKZQo33W0DjF"
      },
      "source": [
        "## xlm-r-bert-base-nli-stsb-mean-tokens"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eGDAtLmn0Dx3"
      },
      "source": [
        "%cd\n",
        "%cd /content/SentEval/examples\n",
        "!python ensemble.py --models xlm-r-bert-base-nli-stsb-mean-tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OipFuOND0KU2"
      },
      "source": [
        "## LaBSE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RjZq7-5n0Kc4"
      },
      "source": [
        "%cd\n",
        "%cd /content/SentEval/examples\n",
        "!python ensemble.py --models LaBSE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99c_Ur8B0TAf"
      },
      "source": [
        "## distilbert-multilingual-nli-stsb-quora-ranking"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LP7SndKX0THQ"
      },
      "source": [
        "%cd\n",
        "%cd /content/SentEval/examples\n",
        "!python ensemble.py --models distilbert-multilingual-nli-stsb-quora-ranking"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjZ8BQZ1rfCM"
      },
      "source": [
        "# Best ensemble combination of models without PCA over STS benchmark 2017 Multilingual\n",
        "\n",
        "## Ensemble of 5 models\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Cv8-UhlrloN"
      },
      "source": [
        "%cd\n",
        "%cd /content/SentEval/examples\n",
        "!python ensemble.py --models distiluse-base-multilingual-cased,xlm-r-distilroberta-base-paraphrase-v1,xlm-r-bert-base-nli-stsb-mean-tokens,LaBSE,distilbert-multilingual-nli-stsb-quora-ranking"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9b8NQsVN4V3z"
      },
      "source": [
        "# Ensemble combination of models without PCA with best ratio result / nº dim over STS benchmark 2017\n",
        "\n",
        "xlm-r-bert-base-nli-stsb-mean-tokens, xlm-r-distilroberta-base-paraphrase-v1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmKsOXfj4WOG"
      },
      "source": [
        "%cd\n",
        "%cd /content/SentEval/examples\n",
        "!python ensemble.py --models xlm-r-distilroberta-base-paraphrase-v1,xlm-r-bert-base-nli-stsb-mean-tokens "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}